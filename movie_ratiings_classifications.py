# -*- coding: utf-8 -*-
"""movie_ratiings_classifications.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s0X7CfxliAFmJ8hb6M8bQUVeN266IadL

##Mengimport Library yang dibutuhkan
"""

# menyiapkan library yang akan digunakan
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense
import matplotlib.pyplot as plt
import numpy as np
import nltk
from nltk.corpus import stopwords
import json
from google.colab import files
import os
import zipfile

"""##Menyiapkan Dataset"""

# mengimport dataset
files.upload()

# mengekstrak file dataset yang berupa zip
local_zip = '/content/Test.csv.zip'
local_unzip = zipfile.ZipFile(local_zip, 'r')
local_unzip.extractall('/content/')
local_unzip.close()

# menyiapkan dataset yang akan digunakan
df = pd.read_csv('/content/Test.csv')
df.tail()

df.info()

"""##Data Preprocessing"""

# mengubah case kata menjadi lower case
df['text'] = df['text'].str.lower()
df.head()

# menghilangkan stop words
nltk.download('stopwords')
stop_word = set(stopwords.words('english'))

df['text'] = df['text'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_word)]))
df.head()

"""##Train dan Test Split"""

# mengubah dataset menjadi numpy array
descriptions = df['text'].values
categories = df['label'].values

# split dataset
x_train, x_test, y_train, y_test = train_test_split(descriptions, categories, test_size=0.2, shuffle=False)

print('Training dataset:\n', x_train.shape, y_train.shape)
print('\nTest dataset:\n', x_test.shape, y_test.shape)

"""##Menerapkan Word Index menggunakan Tokenizer"""

# menghilangkan simbol
filt = '!"#$%&()*+.,-/:;=?@[\]^_`{|}~ '

tokenizer = Tokenizer(num_words=45000, oov_token="", filters=filt)
tokenizer.fit_on_texts(x_train)
tokenizer.fit_on_texts(x_test)

# menyimpan word index kedalam file json
word_index = tokenizer.word_index
print(word_index)
with open('word_index.json', 'w') as fp:
    json.dump(word_index, fp)

# membuat sequences dan menambahkan padding
train_seq = tokenizer.texts_to_sequences(x_train)
test_seq = tokenizer.texts_to_sequences(x_test)
 
train_padded = pad_sequences(train_seq,
                             maxlen=200,
                             padding='post',
                             truncating='post')
test_padded = pad_sequences(test_seq,
                             maxlen=200,
                             padding='post',
                             truncating='post')

print("Train padded : ",train_padded.shape,"|  Test padded : ",test_padded.shape)
print(train_padded)
len(train_padded)

"""##Membangun Model"""

# membangun model sequential
model = tf.keras.Sequential([
    Embedding(45000, 200, input_length=200),
    GlobalAveragePooling1D(),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.summary()

# mengcompile model
# menggunakan 'categorical_crossentropy'
# memanfaatkan optimizer 'adam'
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
    )

# melatih model
num_epochs = 30
history = model.fit(
    train_padded,
    y_train, 
    epochs=num_epochs, 
    validation_data=(test_padded, y_test),
    verbose=2
    )

"""##Plotting Model"""

# ploting accuracy model
# mengevaluasi model dengan plot accuracy dan loss
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, num_epochs), history.history["accuracy"], label="training")
plt.plot(np.arange(0, num_epochs), history.history["val_accuracy"], label="validation")
plt.title("Plot Akurasi")
plt.xlabel("Epoch")
plt.ylabel("Akurasi")
plt.legend()
plt.show()

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, num_epochs), history.history["loss"], label="training")
plt.plot(np.arange(0, num_epochs), history.history["val_loss"], label="validation")
plt.title("Plot Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""##SaveModel"""

model.save("model.h5")

"""##Convert Model ke Tensorflow.js"""

# Install tensorflowjs

!pip install tensorflowjs

# Convert model.h5 to model json
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

!zip -r /content/tfjs_model.zip /content/tfjs_model

files.download('/content/tfjs_model.zip')